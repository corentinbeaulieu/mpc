#%YAML 1.1
# Yaml version 1.1 is needed to avoid 'Map keys must be unique' error
# when parsing yaml file with yamllint.
# Gitlab yaml parser is broken and does not regognise the %YAML header...
# TODO: Uncomment previous header when gitlab has been fixed.
---
#Available stages to run in 'automatic' processing:
# For convenience, implicit stages have been made explicit here...
stages:
  - Installations
  - Basic Testing
  - Regular Testing
  - Applications
  - Benchmarks 
  - Configurations
  - Additionnal Tests
  - Release

# Add clone variables for concurent PIPELINES
variables:
  GL_PIPELINE_PATH: $CI_BUILDS_DIR/GL_$CI_PIPELINE_ID
  GL_BUILD_DIR: $GL_PIPELINE_PATH/build
  GL_INSTALL_DIR: $GL_PIPELINE_PATH/install
  GL_INSTALL_COMMON_OPTS: --enable-debug --with-pmix -j32
  GIT_DEPTH: 1 # No need to clone the whole history
  #GIT_CLONE_PATH: $GL_PIPELINE_PATH/$CI_RUNNER_ID/$CI_CONCURRENT_ID
  PCVS_COMMAND: pcvs -v run --override --profile-path ${GL_PIPELINE_PATH}/mpc_ci/utils/profile/mpc.yml --print errors -S

.pcvs_prepare: &pcvs_prepare
  - export CI_PCVS_DIR="$GL_PIPELINE_PATH/step_$(echo "$CI_JOB_NAME" | sed -e "s/ /_/g")/"
  - test -d "${CI_PCVS_DIR}" && rm -rf "${CI_PCVS_DIR}"
  - mkdir -p "${CI_PCVS_DIR}"
  - cd "${GL_PIPELINE_PATH}"
  - test "x${CI_BRANCH}" != "x" && echo "Using CI branch ${CI_BRANCH}"
  - mkdir mpc_ci 2>"/dev/null"
      && git clone "https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.ccc.ocre.cea.fr/cea-mpc-team/mpc-ci.git" $(test "x${CI_BRANCH}" != "x" && echo "-b ${CI_BRANCH}") "mpc_ci"
      || sleep 5s
  - cd "${CI_PCVS_DIR}"
  - echo "Step run from ${CI_PCVS_DIR}"
  - source ${GL_LOCAL_INSTALL_DIR}/mpcvars.sh
  - echo "MPC loaded from ${GL_LOCAL_INSTALL_DIR}/mpcvars.sh"
  - source "$(ccc_home -u mpc)"/gitlab-runner/spack_organizer_inti/setup.sh
  - spack env activate pcvs
  - module load pmix gnu/13.3.0

# Pipeline launching rules
workflow:
  rules:
    # If pipeline is disabled => don't do
    - if: '$CI_MERGE_REQUEST_LABELS =~ /CI::Disabled/'
      when: never
    # If it is a merge request => do
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    # If it is triggered on the web page => do 
    - if: '$CI_PIPELINE_SOURCE == "web"'
    # If it is a scheduled pipepine => do
    - if: '$CI_PIPELINE_SOURCE == "schedule"'
    # If it is a release
    - if: '$CI_COMMIT_TAG && $CI_COMMIT_BRANCH == "master"'

.uses_slurm: &uses_slurm
  tags:
    - slurm


############################
##### EXTRA ACTIONS ########
############################
#these actions should herit from implicit stages ".pre" and ".post"

# pre-actions to cleanup the machine before the run
# this job will be run in ANY pipeline -> ensure to enable all proper tags
# CAUTION : Otherwise tag-specific runners won't allow to run the whole pipeline because
# both .pre and .post cannot be scheduled (thus, why not using user-defined pre and post
# to avoid them to be run systematically ?)
Env Sanitize:
  stage: .pre
  needs: []
  script:
    - test -d $GL_PIPELINE_PATH && rm -rf $GL_PIPELINE_PATH
    - mkdir -p $GL_PIPELINE_PATH

Resource Relinquishing:
  <<: *uses_slurm
  stage: .post
  when: always
  script:
    - echo "Suppressing ${GL_PIPELINE_PATH} data"
  after_script:
    - rm -rf ${GL_INSTALL_DIR} ${GL_BUILD_DIR}
    - rm -rf ${GL_PIPELINE_PATH}/*

############################
####### CONF STAGE  ########
############################

.conf_run_cmds: &configuration_commands
  - echo "${GL_BUILD_DIR}"
  - echo "${GL_INSTALL_DIR}"
  - rm -rf ${GL_LOCAL_BUILD_DIR} ${GL_LOCAL_INSTALL_DIR}
  - mkdir ${GL_LOCAL_BUILD_DIR}; cd ${GL_LOCAL_BUILD_DIR}
  - source "$(ccc_home -u mpc)"/gitlab-runner/spack_organizer_inti/setup.sh
  - spack env activate pcvs
  - module load pmix gnu/13.3.0
  - ln -sf "$(ccc_home -u mpc)"/mpc_deps "${HOME}"/.mpcdep
  - ${CI_PROJECT_DIR}/installmpc -vv --prefix=${GL_LOCAL_INSTALL_DIR} ${GL_INSTALL_OPTS}
  - spack env deactivate

# Default configuration need pmix to work on inti.
Process Mode Installation:
  stage: Installations
  needs:
    - [Env Sanitize]
  variables:
    GL_LOCAL_BUILD_DIR: ${GL_BUILD_DIR}_process_mode
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_process_mode
    GL_INSTALL_OPTS: ${GL_INSTALL_COMMON_OPTS} --disable-mpc-autopriv --disable-spack
  script:
    - *configuration_commands

Privatization Installation:
  stage: Installations
  needs:
    - [Env Sanitize]
  variables:
    GL_LOCAL_BUILD_DIR: ${GL_BUILD_DIR}_privatization
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_privatization
    GL_INSTALL_OPTS: ${GL_INSTALL_COMMON_OPTS}
  script:
    - *configuration_commands

############################
####### TEST STAGE ########
############################

#
# Basic Testing
# to stop the pipeline early in case of major issue 
# (not able to compile/run a simple test)
#

Process Mode Simple Run:
  <<: *uses_slurm
  stage: Basic Testing
  needs:
    - [Process Mode Installation]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_process_mode
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/trivial/"

Privatization Simple Run:
  <<: *uses_slurm
  stage: Basic Testing
  needs:
    - [Privatization Installation]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_privatization
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/trivial/"

Privatization:
  <<: *uses_slurm
  stage: Additionnal Tests
  needs: [Privatization Installation]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_privatization
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/privatization/"

#
# Regular Testing
# to assess some basic MPC feature
#

Process Mode Lowcomm:
  <<: *uses_slurm
  stage: Regular Testing
  needs:
    - [Process Mode Simple Run]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_process_mode
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/lowcomm/"

Process Mode MPI Simple C:
  <<: *uses_slurm
  stage: Regular Testing
  needs:
    - [Process Mode Simple Run]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_process_mode
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/MPI/simple/c/"

Process Mode MPI Simple Fortran:
  <<: *uses_slurm
  stage: Regular Testing
  needs:
    - [Process Mode Simple Run]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_process_mode
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/MPI/simple/fortran/"

Process Mode OpenMP Tasking examples:
  <<: *uses_slurm
  stage: Regular Testing
  needs:
    - [Process Mode Simple Run]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_process_mode
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/OpenMP/task/examples"

Process Mode OpenMP Tasking cholesky:
  <<: *uses_slurm
  stage: Regular Testing
  needs:
    - [Process Mode Simple Run]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_process_mode
  script:
    - *pcvs_prepare
    - module load llvm
    - module load lapack/mkl
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/OpenMP/task/cholesky"

Privatization Lowcomm:
  <<: *uses_slurm
  stage: Regular Testing
  needs:
    - [Privatization Simple Run]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_privatization
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/lowcomm/"

Privatization MPI Simple C:
  <<: *uses_slurm
  stage: Regular Testing
  needs:
    - [Privatization Simple Run]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_privatization
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/MPI/simple/c/"

Privatization MPI Simple Fortran:
  <<: *uses_slurm
  stage: Regular Testing
  needs:
    - [Privatization Simple Run]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_privatization
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/MPI/simple/fortran/"

Privatization OpenMP Tasking examples:
  <<: *uses_slurm
  stage: Regular Testing
  needs:
    - [Privatization Simple Run]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_privatization
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/OpenMP/task/examples"

Privatization OpenMP Tasking cholesky:
  <<: *uses_slurm
  stage: Regular Testing
  needs:
    - [Privatization Simple Run]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_privatization
  script:
    - *pcvs_prepare
    - module load llvm
    - module load lapack/mkl
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/OpenMP/task/cholesky"

#
# Benchmarks & Applications
#

Process Mode MPI IMB-MPI 2017:
  <<: *uses_slurm
  stage: Benchmarks
  needs:
    - [Process Mode Lowcomm, Process Mode MPI Simple C, Process Mode MPI Simple Fortran]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_process_mode
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/MPI/IMB_2017/check-mpi/"

Process Mode MPI IMB-NBC 2017:
  <<: *uses_slurm
  stage: Benchmarks
  needs:
    - [Process Mode Lowcomm, Process Mode MPI Simple C, Process Mode MPI Simple Fortran]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_process_mode
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/MPI/IMB_2017/check-nbc/"

Process Mode MPI NBC:
  <<: *uses_slurm
  stage: Benchmarks
  needs:
    - [Process Mode Lowcomm, Process Mode MPI Simple C, Process Mode MPI Simple Fortran]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_process_mode
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/MPI/NBC/"

Process Mode NAS-MZ MPI:
  <<: *uses_slurm
  stage: Applications
  needs:
    - [Process Mode Lowcomm, Process Mode MPI Simple C, Process Mode MPI Simple Fortran]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_process_mode
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/hybrid/NAS/MZ-MPI/"

Process Mode Lulesh:
  <<: *uses_slurm
  stage: Applications
  needs:
    - [Process Mode Lowcomm, Process Mode MPI Simple C, Process Mode MPI Simple Fortran]
    - [Process Mode OpenMP Tasking cholesky, Process Mode OpenMP Tasking examples]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_process_mode
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/corals/lulesh-2.0.3/"

Process Mode miniFe:
  <<: *uses_slurm
  stage: Applications
  needs:
    - [Process Mode Lowcomm, Process Mode MPI Simple C, Process Mode MPI Simple Fortran]
    - [Process Mode OpenMP Tasking cholesky, Process Mode OpenMP Tasking examples]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_process_mode
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/corals/miniFe/"

Process Mode OMP NAS-MZ:
  <<: *uses_slurm
  stage: Applications
  needs:
    - [Process Mode OpenMP Tasking cholesky, Process Mode OpenMP Tasking examples]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_process_mode
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/hybrid/NAS/MZ-OMP/"


Privatization MPI IMB-MPI 2017:
  <<: *uses_slurm
  stage: Benchmarks
  needs:
    - [Privatization Lowcomm, Privatization MPI Simple C, Privatization MPI Simple Fortran]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_privatization
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/MPI/IMB_2017/check-mpi/"

Privatization MPI IMB-NBC 2017:
  <<: *uses_slurm
  stage: Benchmarks
  needs:
    - [Privatization Lowcomm, Privatization MPI Simple C, Privatization MPI Simple Fortran]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_privatization
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/MPI/IMB_2017/check-nbc/"

Privatization MPI NBC:
  <<: *uses_slurm
  stage: Benchmarks
  needs:
    - [Privatization Lowcomm, Privatization MPI Simple C, Privatization MPI Simple Fortran]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_privatization
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/MPI/NBC/"

Privatization MPI NAS-MZ:
  <<: *uses_slurm
  stage: Applications
  needs:
    - [Privatization Lowcomm, Privatization MPI Simple C, Privatization MPI Simple Fortran]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_privatization
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/hybrid/NAS/MZ-MPI/"

Privatization Lulesh:
  <<: *uses_slurm
  stage: Applications
  needs:
    - [Privatization Lowcomm, Privatization MPI Simple C, Privatization MPI Simple Fortran]
    - [Privatization OpenMP Tasking cholesky, Privatization OpenMP Tasking examples]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_privatization
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/corals/lulesh-2.0.3/"

Privatization miniFe:
  <<: *uses_slurm
  stage: Applications
  needs:
    - [Privatization Lowcomm, Privatization MPI Simple C, Privatization MPI Simple Fortran]
    - [Privatization OpenMP Tasking cholesky, Privatization OpenMP Tasking examples]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_privatization
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/corals/miniFe/"

Privatization OMP NAS-MZ:
  <<: *uses_slurm
  stage: Applications
  needs:
    - [Privatization OpenMP Tasking cholesky, Privatization OpenMP Tasking examples]
  variables:
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_privatization
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} "${GL_PIPELINE_PATH}/mpc_ci/hybrid/NAS/MZ-OMP/"


# Configurations
Configurations:
  stage: Configurations
  needs: [Env Sanitize]
  parallel:
    matrix:
      - CONFIGURATION_NAME: DisableLowcomm
        GL_INSTALL_OPTS: ${GL_INSTALL_COMMON_OPTS} --disable-mpc-autopriv --mpc-option="--disable-lowcomm"
      - CONFIGURATION_NAME: DisableMPI
        GL_INSTALL_OPTS: ${GL_INSTALL_COMMON_OPTS} --disable-mpc-autopriv --mpc-option="--disable-mpi"
      - CONFIGURATION_NAME: DisableThreads
        GL_INSTALL_OPTS: ${GL_INSTALL_COMMON_OPTS} --disable-mpc-autopriv --mpc-option="--disable-threads"
      - CONFIGURATION_NAME: DisablePMIx
        GL_INSTALL_OPTS: --disable-mpc-autopriv --enable-debug -j32
      - CONFIGURATION_NAME: DisableMPCAlloc
        GL_INSTALL_OPTS: ${GL_INSTALL_COMMON_OPTS} --disable-mpcalloc
  variables:
    GL_LOCAL_BUILD_DIR: ${GL_BUILD_DIR}_${CONFIGURATION_NAME}
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_${CONFIGURATION_NAME}
  script:
    - *configuration_commands
  after_script:
    - rm -rf ${GL_LOCAL_INSTALL_DIR} ${GL_LOCAL_BUILD_DIR}

# FIXME: We have to update Workshare for this test to pass 
# MPC Compilation Workshare:
#   stage: Configurations
#   variables:
#     GL_LOCAL_BUILD_DIR: ${GL_BUILD_DIR}_workshare
#     GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_workshare
#     GL_INSTALL_OPTS: --enable-workshare --enable-debug
#   allow_failure: true
#   script:
#     - *configuration_commands

###################################
####### MANUALLY TRIGGERED ########
###################################
#This section gathers jobs not scheduled to be part of the standard pipeline.
#They could be run manually or periodically.

#When triggered through a variable, MPC is built in containers with fixed software stack (base compiler, libc, etc...)
# to ensure MPC to compile in a large variety of environments

Debian Stretch:
  stage: Additionnal Tests
  script: "docker run --rm -it paratoolsfrance/mpc-env:debian-stretch $HOME/docker/run_installmpc.sh"
  tags:
    - "docker"
  only:
    variables:
      - $MPC_CI_MODE == "multibuild"

Centos 7:
  stage: Additionnal Tests
  script: "docker run --rm -it paratoolsfrance/mpc-env:centos-7 $HOME/docker/run_installmpc.sh"
  tags:
    - "docker"
  only:
    variables:
      - $MPC_CI_MODE == "multibuild"

release:
  stage: Release
  rules:
    - if: $CI_COMMIT_TAG && $CI_COMMIT_BRANCH == "master"
  script: utils/release_all.sh $(ccc_home -u mpc_nr)/releases/$CI_COMMIT_TAG
  release:
    tag_name: '$CI_COMMIT_TAG'
    description: '$CI_COMMIT_TAG_MESSAGE'
  artifacts:
    paths: [$(ccc_home -u mpc_nr)/releases/$CI_COMMIT_TAG]
